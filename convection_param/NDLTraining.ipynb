{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c42626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "from os import DirEntry\n",
    "import sys\n",
    "import re\n",
    "from itertools import chain\n",
    "from numba import jit, njit\n",
    "from HelperFuncs import *\n",
    "from convection_param.ConvParamFuncs import *\n",
    "from convection_param.GetTrainingData import *\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "xr.set_options(display_style=\"html\");\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a782af1",
   "metadata": {},
   "source": [
    "This notebooks is for training of different non-deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2eb4f86-6ce1-4e26-a03e-27e435550efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8ab28c-6a2d-4d0f-a80b-23d3636b8886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_train', 'X_val', 'X_test', 'Y_train', 'Y_val', 'Y_test', 'X_expl', 'Y_expl', 'train_coords', 'val_coords', 'test_coords']\n"
     ]
    }
   ],
   "source": [
    "# data = np.load('../Processed/TrainData/R2B5_vcg_20221118-153949.npz')\n",
    "# data = np.load('../local_data/TrainData/20230111-165428-R2B5_y13y16_vcg-fluxes_rho_fluct.npz')\n",
    "data = np.load('../local_data/TrainData/20230131-171851-R2B5_y13y16_vcg-fluxes_rho_fluct.npz')\n",
    "\n",
    "print(data.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80612a42-c9e2-45a2-baf9-37ff969cba0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True]\n",
      "X_train shape:  (1613616, 23, 9)\n",
      "X_val shape:  (201702, 23, 9)\n",
      "X_test shape:  (201702, 23, 9)\n",
      "Y_train shape:  (1613616, 189)\n",
      "Y_val shape:  (201702, 189)\n",
      "Y_test shape:  (201702, 189)\n",
      "len X_expl 207\n",
      "len Y_expl 189\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, Y_train, Y_val, Y_test, X_expl, Y_expl = \\\n",
    "data['X_train'], data['X_val'], data['X_test'], data['Y_train'], data['Y_val'], data['Y_test'], data['X_expl'], data['Y_expl']\n",
    "\n",
    "from convection_param.HelperFuncs import unique_unsorted\n",
    "\n",
    "# vars_to_neglect = ['qr','qi','qs']\n",
    "# vars_to_neglect = ['qr','qs']\n",
    "vars_to_neglect = []\n",
    "vars_to_neglect_mask = ~np.isin(unique_unsorted([e[0] for e in X_expl]), vars_to_neglect)\n",
    "print(vars_to_neglect_mask)\n",
    "X_train = X_train[:,:,vars_to_neglect_mask]\n",
    "X_val = X_val[:,:,vars_to_neglect_mask]\n",
    "X_test = X_test[:,:,vars_to_neglect_mask]\n",
    "X_expl = np.array([e for e in X_expl if e[0] not in vars_to_neglect])\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('Y_train shape: ', Y_train.shape)\n",
    "print('Y_val shape: ', Y_val.shape)\n",
    "print('Y_test shape: ', Y_test.shape)\n",
    "print('len X_expl', len(X_expl))\n",
    "print('len Y_expl', len(Y_expl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a106b0-b8b7-47b5-86e2-7e63ea14da29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1613616, 207)\n",
      "X_val shape:  (201702, 207)\n",
      "X_test shape:  (201702, 207)\n"
     ]
    }
   ],
   "source": [
    "# If channeled data is input\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_train = X_train[~np.isclose(X_train.std(axis=0), 0)]\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "# X_val = X_val[~np.isclose(X_val.std(axis=0), 0)]\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "# X_test = X_test[~np.isclose(X_test.std(axis=0), 0)]\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('X_test shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9efd35",
   "metadata": {},
   "source": [
    "#### Use multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e869643c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "def save_models(model_list, model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f'Path {model_path} already exists')\n",
    "        return\n",
    "    os.mkdir(model_path)\n",
    "    for model in model_list:\n",
    "        pkl_filename = os.path.join(model_path, get_sk_model_name(model) + '.joblib')\n",
    "        print(f'Saving to {pkl_filename}')\n",
    "        dump(model, pkl_filename)\n",
    "\n",
    "            \n",
    "def load_models(path, only_paths=False):\n",
    "    if isinstance(path, list):\n",
    "        models = []\n",
    "        for p in path:\n",
    "            m = load_models(p, only_paths)\n",
    "            models.extend(m)\n",
    "    else:\n",
    "        files = sorted([f for f in os.scandir(path)], key=lambda f: f.name)\n",
    "        models = []\n",
    "        files = [file for file in files if file.is_file()]\n",
    "        for file in files:\n",
    "            if 'joblib' in file.path:\n",
    "                if only_paths:\n",
    "                    print(f'Adding path: {file.path}')\n",
    "                    models.append(file)\n",
    "                else:\n",
    "                    print(f'Loading from file {file.path}')\n",
    "                    models.append(load(file.path))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971b863",
   "metadata": {},
   "source": [
    "### Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e138264b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Baseline:\n",
    "    def __init__(self):\n",
    "        self.Y_median = None\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.Y_median = np.median(Y, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.tile(self.Y_median[None,:], (X.shape[0],1))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Baseline()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde4b460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    # ensemble.RandomForestRegressor(max_features=0.3, min_samples_leaf=0.01, n_jobs=-4),\n",
    "    linear_model.LinearRegression(),\n",
    "    ensemble.RandomForestRegressor(n_jobs=-2),\n",
    "    # linear_model.Ridge(alpha=1),\n",
    "    linear_model.Ridge(),\n",
    "    # linear_model.MultiTaskLasso(alpha=1.),\n",
    "    linear_model.MultiTaskLasso(),\n",
    "    # linear_model.MultiTaskElasticNet(alpha=1, l1_ratio=0.5),\n",
    "    linear_model.MultiTaskElasticNet(),\n",
    "    # KernelRidge(kernel='rbf', alpha=1),\n",
    "    KernelRidge(),\n",
    "    # ensemble.ExtraTreesRegressor(max_features=0.3, n_jobs=-4),\n",
    "    ensemble.ExtraTreesRegressor(n_jobs=-2),\n",
    "    MultiOutputRegressor(ensemble.GradientBoostingRegressor(), n_jobs=-2),\n",
    "    MultiOutputRegressor(SVR(), n_jobs=-2),\n",
    "    # GaussianProcessRegressor(),#ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(50.0, length_scale_bounds=\"fixed\")), # Found by grid search CV on 20% of training data\n",
    "    # Baseline(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419b3ffd-eb4e-4ed2-a471-2143bf497d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rf_tree_depths(rf):\n",
    "    return [estimator.get_depth() for estimator in rf]\n",
    "\n",
    "def get_max_gbr_tree_depth(gbr):\n",
    "    return max([pred.get_max_depth() for estimator in gbr.estimators_ for predictor_list in estimator._predictors for pred in predictor_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939dc558-9177-4d9c-a047-3ddca0a3b638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/0.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/1.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/2.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/3.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/5.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/6.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/8.joblib\n",
      "Loading from file Models/ClassicHPO/20230419-144242-nsamples143401-5days/9.joblib\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "gcv_models = load_models('Models/ClassicHPO/20230419-144242-nsamples143401-5days/')\n",
    "# gcv_models = load_models('Models/ClassicHPO/20230504-171743-nsamples143401-5days_woqrqs/')\n",
    "gcv_models = gcv_models[1:]\n",
    "\n",
    "# gcv_models = [load('Models/ClassicHPO/20230419-144242-nsamples143401-5days/3.joblib')]\n",
    "models = [deepcopy(m.best_estimator_) for m in gcv_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cd8653-06c5-46b1-883b-015d0cc70413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiTaskElasticNet(alpha=0.1, l1_ratio=0.1)\n",
      "39123\n",
      "189\n",
      "39312\n"
     ]
    }
   ],
   "source": [
    "m = models[2]\n",
    "print(m)\n",
    "print(m.coef_.size)\n",
    "print(m.intercept_.size)\n",
    "print(m.coef_.size + m.intercept_.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d38f0b2-f36e-40ad-b5ff-e3ebcce5118a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "54\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "max_depth_rf = np.max(get_rf_tree_depths(models[3]))\n",
    "max_depth_et = np.max(get_rf_tree_depths(models[4]))\n",
    "max_depth_gbr = get_max_gbr_tree_depth(models[5])\n",
    "\n",
    "models[3].set_params(max_depth=max_depth_rf)\n",
    "models[4].set_params(max_depth=max_depth_et)\n",
    "models[5].estimator.set_params(max_depth=max_depth_gbr)\n",
    "\n",
    "print(max_depth_rf)\n",
    "print(max_depth_et)\n",
    "print(max_depth_gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebf6b195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge\n"
     ]
    }
   ],
   "source": [
    "def get_sk_model_name(model):\n",
    "    if isinstance(model, DirEntry):\n",
    "        model = model.path\n",
    "    model = str(model).split('/')[-1]\n",
    "    name = model.split('(')[0]\n",
    "    if name=='MultiOutputRegressor' or name=='GridSearchCV' or name=='RandomizedSearchCV':\n",
    "        name = re.search('estimator=(.*)\\(', str(model)).group(1)\n",
    "    name = name.replace('.joblib', '')\n",
    "    return name\n",
    "\n",
    "print(get_sk_model_name(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fcb029-b22c-4c24-9c16-244d5888a5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1613616 samples\n"
     ]
    }
   ],
   "source": [
    "npr.seed(1245845)\n",
    "sample_frac = 1\n",
    "sample_idx = npr.choice(X_train.shape[0], size=int(sample_frac*X_train.shape[0]), replace=False)#np.s_[:]#\n",
    "print(f'Using {len(sample_idx)} samples')\n",
    "\n",
    "X_train_sample = X_train[sample_idx]\n",
    "Y_train_sample = Y_train[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686dbce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ridge', 'MultiTaskLasso', 'MultiTaskElasticNet', 'RandomForestRegressor', 'ExtraTreesRegressor', 'HistGradientBoostingRegressor', 'LassoLars']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aa029ed464444bb9234c9bb48c9add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model: Ridge\n",
      "(1613616, 207)\n",
      "(1613616, 189)\n",
      "fit_time = 7.986676931381226s\n",
      "Saving to Models/Optimized/20230829-100912_1samples/Ridge.joblib\n",
      "R2_test(Ridge): -2.0760515544344837\n",
      "R2_val(Ridge): -2.631908964269372\n",
      "Using Model: MultiTaskElasticNet\n",
      "(1613616, 207)\n",
      "(1613616, 189)\n"
     ]
    }
   ],
   "source": [
    "model_name_list = [get_sk_model_name(model) for model in models]\n",
    "print(model_name_list)\n",
    "Y_expl_names = ['_'.join(map(str, expl)) for expl in Y_expl]\n",
    "\n",
    "path = 'Models/Optimized/'\n",
    "model_path = os.path.join(path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) + f'_{sample_frac}samples'\n",
    "perform_fit = True\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "fit_times = []\n",
    "sample_sizes = {}\n",
    "\n",
    "def comp_metrics(Y_test, Y_test_hat):\n",
    "    RMSE = np.sqrt(mean_squared_error(Y_test, Y_test_hat))\n",
    "    R2 = r2_score(Y_test, Y_test_hat, multioutput='raw_values')\n",
    "    model_var = np.var(Y_test_hat, axis=0)\n",
    "    return RMSE, R2, model_var\n",
    "\n",
    "for model, name in tqdm(list(zip(models, model_name_list))[0:3:2]):\n",
    "    df_RMSE = pd.DataFrame(columns = [name], index = Y_expl_names)\n",
    "    df_R2 = pd.DataFrame(columns = [name], index = Y_expl_names)\n",
    "    df_var = pd.DataFrame(columns = [name], index = Y_expl_names)\n",
    "    if isinstance(model, DirEntry):\n",
    "        model = load(model)\n",
    "    print(f'Using Model: {name}')\n",
    "    \n",
    "    if perform_fit:\n",
    "        t0 = time.time()\n",
    "        print(X_train_sample.shape)\n",
    "        print(Y_train_sample.shape)\n",
    "        model.fit(X_train_sample, Y_train_sample)\n",
    "        fit_time = time.time() - t0\n",
    "        print(f'fit_time = {fit_time}s')\n",
    "        fit_times.append(fit_time)\n",
    "        sample_sizes[name] = len(X_train_sample)\n",
    "        pkl_filename = os.path.join(model_path, get_sk_model_name(model) + '.joblib')\n",
    "        print(f'Saving to {pkl_filename}')\n",
    "        dump(model, pkl_filename)\n",
    "    \n",
    "    Y_test_hat = model.predict(X_test)\n",
    "    \n",
    "    RMSE, R2, var = comp_metrics(Y_test, Y_test_hat)\n",
    "    print(f'R2_test({name}): {np.mean(R2)}')\n",
    "    df_RMSE[name] = RMSE\n",
    "    df_R2[name] = R2\n",
    "    df_var[name] = var\n",
    "    df_RMSE.to_csv(os.path.join(model_path, f'df_RMSE_test_{name}.csv'))\n",
    "    df_R2.to_csv(os.path.join(model_path, f'df_R2_test_{name}.csv'))\n",
    "    df_var.to_csv(os.path.join(model_path, f'df_var_test_{name}.csv'))\n",
    "\n",
    "    Y_val_hat = model.predict(X_val)\n",
    "\n",
    "    RMSE, R2, var = comp_metrics(Y_val, Y_val_hat)\n",
    "    print(f'R2_val({name}): {np.mean(R2)}')\n",
    "    df_RMSE[name] = RMSE\n",
    "    df_R2[name] = R2\n",
    "    df_var[name] = var\n",
    "    df_RMSE.to_csv(os.path.join(model_path, f'df_RMSE_val_{name}.csv'))\n",
    "    df_R2.to_csv(os.path.join(model_path, f'df_R2_val_{name}.csv'))\n",
    "    df_var.to_csv(os.path.join(model_path, f'df_var_val_{name}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a20926-80a1-42a5-ade8-5133d438956e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if perform_fit:\n",
    "    with open(os.path.join(model_path, 'fit_times.csv'), 'w') as f:\n",
    "        f.write('#model,fit_time\\n')\n",
    "        f.write('#,seconds\\n')\n",
    "        fitted_models_size = len(fit_times)\n",
    "        for name, fit_time in zip(model_name_list[:fitted_models_size], fit_times):\n",
    "            f.write(f'{name},{fit_time}\\n')\n",
    "    \n",
    "    pd.DataFrame(sample_sizes, index=['#samples']).to_csv(os.path.join(model_path, 'sample_sizes.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93da17-4edd-433b-bac4-5c8cb1099dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ed9b5-29e0-4942-9b8a-c1e73c3adc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.8 (conda)",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
