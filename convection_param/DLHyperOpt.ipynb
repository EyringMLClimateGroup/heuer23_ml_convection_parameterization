{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdda0f8-a32c-49c1-b790-f8dbe98e561f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13540e2e",
   "metadata": {},
   "source": [
    "This notebooks performs an HPO with `ray-tune` and `pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751271e2-8d8e-40de-9567-5c1c785fc9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_trainval_data(data_file, config):\n",
    "    datasets = torch.load(data_file)\n",
    "\n",
    "    train_data = datasets[0]\n",
    "    val_data = datasets[1]\n",
    "    # test_data = datasets[2]\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=1024, shuffle=False)\n",
    "    # test_dataloader = DataLoader(test_data, batch_size=1024, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# data_file = '../local_data/TrainData/20230111-165428-R2B5_y13y16_vcg-fluxes_rho_fluct.torch_data'\n",
    "# tloader,vloader = load_trainval_data(data_file, {'batch_size': 512})\n",
    "# print(len(tloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e610511-128a-41b6-b6bc-c50ead267e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e353b8df-a698-4b82-acb3-b2bd5e1c5b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from convection_param.NetworksTorch import Sequential, Unet, ResDNN, SeqConv\n",
    "\n",
    "def train(config, epochs, checkpoint_dir=None, data_file=None, model=None, save_model=False):\n",
    "    \n",
    "    print('Starting hpo training run')\n",
    "    if model == 'seq':\n",
    "        model = Sequential(input_dim=9*23,\n",
    "                           output_dim=189,\n",
    "                           n_hidden=config['n_hidden'],\n",
    "                           n_layers=config['n_layers'],\n",
    "                           activation=config['activation'],\n",
    "                           bn=config['bn'])\n",
    "    elif model == 'unet':\n",
    "        model = Unet(n_channels=9,\n",
    "                     n_classes=8,\n",
    "                     output_channels_total=189,\n",
    "                     n_levels=config['n_levels'],\n",
    "                     n_features=config['n_features'],\n",
    "                     column_height=23,\n",
    "                     linear=False,\n",
    "                     activation=config['activation'],\n",
    "                     bn1=config['bn1'],\n",
    "                     bn2=config['bn2'])\n",
    "    elif model == 'resdnn':\n",
    "        model = ResDNN(in_size=23*9,\n",
    "                       out_size=189,\n",
    "                       n_neurons=config['n_neurons'],\n",
    "                       bn=config['batch_norm'],\n",
    "                       n_layers_per_block=config['layers_per_block'],\n",
    "                       n_levels=config['n_lvls'],\n",
    "                       activation=config['activation'])\n",
    "    elif model == 'seqconv':\n",
    "        model = SeqConv(n_channels=9,\n",
    "                        n_feature_channels=config['n_channels'],\n",
    "                        column_height=23,\n",
    "                        n_hidden=config['n_hidden'],\n",
    "                        n_layers=config['n_layers'],\n",
    "                        output_dim=189,\n",
    "                        activation=config['activation'],\n",
    "                        kernel_size=config['kernel_size'])\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            \n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])#, lr=0.0003)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainloader, valloader = load_trainval_data(data_file, config)\n",
    "    train_steps = len(trainloader)\n",
    "    val_steps = len(valloader)\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        model.train()\n",
    "        # for batch, (X, y) in enumerate(dataloader):\n",
    "        train_loss = 0\n",
    "        for batch, (X, y) in enumerate(trainloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss /= train_steps\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        for X, y in valloader:\n",
    "            with torch.no_grad():\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        val_loss /= val_steps\n",
    "        \n",
    "        tune.report(train_loss=train_loss, val_loss=val_loss)\n",
    "        # tune.report(train_loss=train_loss.cpu().numpy(), val_loss=val_loss.cpu().numpy())\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda26181-de88-4835-b5ea-6db222e5de1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64, 128, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "def feat_lvl_dependence(n_lvl):\n",
    "    max_channel_amount = 2048\n",
    "    max_feat = max_channel_amount/(2**n_lvl)\n",
    "    max_idx = int(np.floor(np.log2(max_feat)))\n",
    "    feat_list = 2**np.arange(max_idx-2,max_idx+1)\n",
    "    # return npr.choice(feat_list)\n",
    "    return feat_list\n",
    "\n",
    "feat_lvl_dependence(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c635058e-5158-4479-8ce7-8cac1cc3cda1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "67200\n",
      "19200\n"
     ]
    }
   ],
   "source": [
    "config_seq = {\n",
    "    \"batch_size\": tune.choice([512,1024]),\n",
    "    # \"n_hidden\": tune.randint(100, 1500),\n",
    "    'n_hidden': tune.choice([16,32,64,128,256,512,1024,2048]),\n",
    "    \"activation\": tune.choice([F.relu, F.selu, F.gelu, F.sigmoid, F.leaky_relu]),\n",
    "    \"bn\": tune.choice([True,False]),\n",
    "    'n_layers': tune.choice([1,2,3,4,5,6]),\n",
    "    'lr': tune.choice([0.1,0.01,0.001,0.0003,0.0001]),\n",
    "}\n",
    "current_best_params_seq = [{\n",
    "  \"activation\": F.leaky_relu,\n",
    "  \"batch_size\": 1024,\n",
    "  \"bn\": True,\n",
    "  \"n_hidden\": 2048,\n",
    "  \"n_layers\": 6,\n",
    "  'lr': 0.0003,\n",
    "}]\n",
    "\n",
    "config_unet = {\n",
    "    \"batch_size\": tune.choice([512,1024]),\n",
    "    # \"n_levels\": tune.randint(3,6),\n",
    "    \"n_levels\": tune.choice([2,3,4,5]),\n",
    "    # \"n_features\": tune.choice([16,32,64,128,264]),\n",
    "    \"n_features\": tune.sample_from(lambda spec: feat_lvl_dependence(spec.config.n_levels)),\n",
    "    \"bn1\": tune.choice([True,False]),\n",
    "    \"bn2\": tune.choice([True,False]),\n",
    "    \"activation\": tune.choice([F.relu, F.selu, F.gelu, F.sigmoid, F.leaky_relu]),\n",
    "    'lr': tune.choice([0.1, 0.01,0.001,0.0003,0.0001]),\n",
    "}\n",
    "current_best_params_unet = [{\n",
    "             \"batch_size\":512,\n",
    "             \"n_levels\":2,\n",
    "             \"n_features\":512,\n",
    "             \"bn1\": False,\n",
    "             \"bn2\": False,\n",
    "             \"activation\": F.leaky_relu,\n",
    "             'lr': 0.0001,\n",
    "}]\n",
    "\n",
    "config_resdnn = {\n",
    "    'batch_size': tune.choice([512,1024]),\n",
    "    # 'n_neurons': tune.randint(100,1500),\n",
    "    'n_neurons': tune.choice([16,32,64,128,256,512,1024,2048]),\n",
    "    'batch_norm': tune.choice([True, False]),\n",
    "    'layers_per_block': tune.choice([1,2,3,4]),\n",
    "    'n_lvls': tune.choice([2,4,8,10,14,16]),\n",
    "    'activation': tune.choice([nn.ReLU(), nn.SELU(), nn.GELU(), nn.Sigmoid(), nn.LeakyReLU()]),\n",
    "    'lr': tune.choice([0.1, 0.01,0.001,0.0003,0.0001]),\n",
    "}\n",
    "current_best_params_resdnn = [{\n",
    "    'batch_size':1024,\n",
    "    'n_neurons': 2048,\n",
    "    'batch_norm': True,\n",
    "    'layers_per_block': 1,\n",
    "    'n_lvls': 10,\n",
    "    'activation': nn.ReLU(),\n",
    "    'lr': 0.0003,\n",
    "}]\n",
    "\n",
    "config_seq_conv = {\n",
    "    'batch_size': tune.choice([512,1024]),\n",
    "    'n_channels': tune.choice([1,8,32,64,256,512,1024]),\n",
    "    # 'n_hidden': tune.randint(100,1500),\n",
    "    'n_hidden': tune.choice([16,32,64,128,256,512,1024,2048]),\n",
    "    'n_layers': tune.choice([0,1,2,3,4,5]),\n",
    "    'kernel_size': tune.choice([2,3,4,5]),\n",
    "    \"activation\": tune.choice([F.relu, F.selu, F.gelu, F.sigmoid, F.leaky_relu]),\n",
    "    'lr': tune.choice([0.1, 0.01,0.001,0.0003,0.0001]),\n",
    "}\n",
    "current_best_params_seq_conv = [{\n",
    "    'batch_size': 512,\n",
    "    'n_channels': 1024,\n",
    "    'n_hidden': 1024,\n",
    "    'n_layers': 1,\n",
    "    'kernel_size': 5,\n",
    "    'activation': F.gelu,\n",
    "    'lr': 0.0001,\n",
    "}]\n",
    "\n",
    "def get_sp_size(search_space):\n",
    "    result = 1\n",
    "    for key,space in search_space.items():\n",
    "        result *= len(space)\n",
    "    return result\n",
    "\n",
    "print(get_sp_size(config_seq))\n",
    "print(get_sp_size(config_seq_conv))\n",
    "print(get_sp_size(config_resdnn))\n",
    "# get_sp_size(config_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83a1e1-97a4-41f0-aa74-52d72edbbe3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = 'unet'#'seq'#'resdnn'#'seqconv'#\n",
    "log_dir = f'../logs_ray_torch/BestModelRetrain/{model}'\n",
    "model_conf_map = {'seq': config_seq,\n",
    "                  'seqconv': config_seq_conv,\n",
    "                  'resdnn': config_resdnn,\n",
    "                  'unet': config_unet}\n",
    "\n",
    "def main(num_samples=10, max_num_epochs=10, grace_period=10, gpus_per_trial=1):\n",
    "    data_file = '../local_data/TrainData/20230131-171851-R2B5_y13y16_vcg-fluxes_rho_fluct.torch_data'\n",
    "    metric='val_loss'\n",
    "    mode='min'\n",
    "        \n",
    "    # hyperopt_search = HyperOptSearch(\n",
    "    #     metric=metric, mode=mode, space=model_conf_map[model])#,\n",
    "        # points_to_evaluate=current_best_params_resdnn)\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=metric,\n",
    "        mode=mode,\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=grace_period,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"val_loss\"])\n",
    "    result = tune.run(\n",
    "        partial(train, epochs=max_num_epochs, data_file=data_file, model=model),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=model_conf_map[model],\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        # search_alg=hyperopt_search,\n",
    "        progress_reporter=reporter,\n",
    "        local_dir=log_dir)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"val_loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"val_loss\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ray.init(dashboard_host = '0.0.0.0')\n",
    "    # main(num_samples=50, max_num_epochs=100, grace_period=30, gpus_per_trial=1)\n",
    "    main(num_samples=4, max_num_epochs=10, grace_period=10, gpus_per_trial=1)\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.10_torch)",
   "language": "python",
   "name": "py3.10_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
